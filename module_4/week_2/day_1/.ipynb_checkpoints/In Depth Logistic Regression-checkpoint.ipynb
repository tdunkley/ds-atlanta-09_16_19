{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[New Feature Google Colab](https://colab.research.google.com/notebooks/data_table.ipynb#scrollTo=jcQEX_3vHOUz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Logistic Regression Continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy.stats as st\n",
    "import statsmodels.datasets\n",
    "%matplotlib inline\n",
    "sns.set(font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc=pd.read_csv('breast_cancer_scikit_onehot_dataset.csv')\n",
    "bc.head()\n",
    "target = bc['class'].map(lambda x: 1 if x == 4 else 0).values \n",
    "target = pd.Series(target)\n",
    "predictor=bc.drop(columns=['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictor, target, test_size=0.3,random_state=9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a logistic Classifier\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "#Grid Search\n",
    "C= np.arange(.000001,2,.01)\n",
    "parameters = {'penalty':['l1', 'l2'], 'C':C,'class_weight':['balanced']}\n",
    "clf = GridSearchCV(log_reg, parameters, cv=5,verbose=0)\n",
    "\n",
    "# Obtain best model\n",
    "best_model = clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yprob = best_model.predict_proba(X_test)\n",
    "yprob[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting Thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Logistic Regression gives probability predictions for each class, in addition to the final classification. \n",
    "- By default, threshold for the prediction is set to 0.5, but we can adjust that threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = []\n",
    "for item in best_model.predict_proba(X_test):\n",
    "    if item[1] >= .40:\n",
    "        predicts.append(1)\n",
    "    else:\n",
    "        predicts.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = pd.DataFrame(confusion_matrix(y_test, predicts),\n",
    "                           index = ['actual 0', 'actual 1'], \n",
    "                           columns = ['predicted 0', 'predicted 1'])\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## accuracy\n",
    "(conf_matrix['predicted 0'][0] + conf_matrix['predicted 1'][1]) / len(predicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does this change the Precision, Recall, and F1-Score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability, odds,log, log-odds\n",
    "\n",
    "$$probability = \\frac {one\\ outcome} {all\\ outcomes}$$\n",
    "\n",
    "$$odds = \\frac {one\\ outcome} {all\\ other\\ outcomes}$$\n",
    "\n",
    "Examples:\n",
    "\n",
    "- Dice roll of 1: probability = 1/6, odds = 1/5\n",
    "\n",
    "\n",
    "- Even dice roll: probability = 3/6, odds = 3/3 = 1\n",
    "\n",
    "\n",
    "- Dice roll less than 5: probability = 4/6, odds = 4/2 = 2\n",
    "\n",
    "$$odds = \\frac {probability} {1 - probability}$$\n",
    "\n",
    "$$probability = \\frac {odds} {1 + odds}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "logit = sm.Logit(y_train, X_train)\n",
    "\n",
    "# fit the model\n",
    "logit_model = logit.fit()\n",
    "\n",
    "logit_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Positive coefficients increase the log-odds of the response (and thus increase the probability)\n",
    "- Negative coefficients decrease the log-odds of the response (and thus decrease the probability)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Odds Ratio\n",
    "\n",
    "- **Odds Ratio** is the exponentiated coefficient, and can be interpreted as the multiplicative change in the odds for a one unit change in the predictor variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odds ratio\n",
    "np.exp(logit_model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odds ratios and 95% CI\n",
    "params = logit_model.params\n",
    "conf = logit_model.conf_int()\n",
    "conf['OR'] = params\n",
    "conf.columns = ['2.5%', '97.5%', 'OR']\n",
    "np.exp(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Logistic Regression with Other Models\n",
    "\n",
    "Advantages of logistic regression:\n",
    "\n",
    "- Highly interpretable (if you remember how)\n",
    "- Model training and prediction are fast\n",
    "- Not many parameters to tune\n",
    "- Can perform well with a small number of observations\n",
    "- Outputs well-calibrated predicted probabilities\n",
    "\n",
    "Disadvantages of logistic regression:\n",
    "\n",
    "- Presumes a linear relationship between the features and the log-odds of the response\n",
    "- Performance is (generally) not competitive with the best supervised learning methods\n",
    "- Can't automatically learn feature interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Maximum Likelihood\n",
    "- [MLE Video](https://www.youtube.com/watch?v=XepXtl9YKwc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Procedure of finding the value of one or more parameters for a given statistic which makes the known likelihood distribution a maximum. \n",
    "- Common way of thinking about optimizing parameters in statistics and machine learning.   \n",
    "- Conceptually when working with a probabilistic model with **unknown parameters**, the parameters which make the data have the highest probability are the most likely ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Suppose we have a model with parameters $\\boldsymbol{\\theta}$ and a collection of data points $X$.  \n",
    "\n",
    "We want to find the most likely value for the parameters of our model, that means we want to find\n",
    "\n",
    "$$\n",
    "\\mathop{\\mathrm{argmax}} P(\\boldsymbol{\\theta}\\mid X).\n",
    "$$\n",
    "\n",
    "By Bayes' rule, this is the same thing as\n",
    "\n",
    "$$\n",
    "\\mathop{\\mathrm{argmax}} \\frac{P(X \\mid \\boldsymbol{\\theta})P(\\boldsymbol{\\theta})}{P(X)}.\n",
    "$$\n",
    "\n",
    "The expression $P(X)$, a parameter agnostic probability of generating the data, does not depend on $\\boldsymbol{\\theta}$ at all, and so can be dropped without changing the best choice of $\\boldsymbol{\\theta}$.  Similarly, we may now posit that we have no prior assumption on which set of parameters are better than any others, so we may declare that $P(\\boldsymbol{\\theta})$ does not depend on theta either!  This, for instance, makes sense in a coin flipping example where the probability a coin comes up heads could be any value in $[0,1]$ without any prior belief it is fair or not (often referred to as an *uninformative prior*).  Thus we see that our application of Bayes' rule shows that our best choice of $\\boldsymbol{\\theta}$ is the maximum likelihood estimate for $\\boldsymbol{\\theta}$:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}} = \\mathop{\\mathrm{argmax}} _ {\\boldsymbol{\\theta}} P(X \\mid \\boldsymbol{\\theta}).\n",
    "$$\n",
    "\n",
    "As a matter of common terminology, the probability of the data given the parameters ($P(X \\mid \\boldsymbol{\\theta})$) is referred to as the *likelihood*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Exponential Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = statsmodels.datasets.heart.load_pandas().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A censor of 0 means that the patient was alive at the end of the study, and thus we don't know the exact survival time. We only know that the patient survived at least the indicated number of days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.censors == 1]\n",
    "survival = data.survival\n",
    "survival.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of the Survival Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "ax1.plot(sorted(survival)[::-1], 'o')\n",
    "ax1.set_xlabel('Patient')\n",
    "ax1.set_ylabel('Survival time (days)')\n",
    "\n",
    "ax2.hist(survival, bins=15)\n",
    "ax2.set_xlabel('Survival time (days)')\n",
    "ax2.set_ylabel('Number of patients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's try to fit an exponential distribution to the data. \n",
    "- According to this model, $S$ (number of days of survival) is an exponential random variable with the parameter $\\lambda$ , and the observations ${s_i}$ are sampled from this distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the sample mean as:\n",
    "$$\\overline s = \\frac 1 n \\sum s_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood function of an exponential distribution is as follows, by definition\n",
    "\n",
    "$$\\mathcal{L}(\\lambda, \\{s_i\\}) = P(\\{s_i\\} \\mid \\lambda) = \\lambda^n \\exp\\left(-\\lambda n \\overline s\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum likelihood estimate for the rate parameter is, by definition, the value ${λ}$ that maximizes the likelihood function. In other words, it is the parameter that maximizes the probability of observing the data, assuming that the observations are sampled from an exponential distribution.\n",
    "\n",
    "Here, it can be shown that the likelihood function has a maximum value when $λ= \\frac{1}{\\overline s}$, which is the maximum likelihood estimate for the rate parameter. Let's compute this parameter numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smean = survival.mean()\n",
    "rate = 1. / smean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To compare the fitted exponential distribution to the data, \n",
    "#we first need to generate linearly spaced values for the x-axis (days):\n",
    "smax = survival.max()\n",
    "days = np.linspace(0., smax, 1000)\n",
    "# bin size: interval between two\n",
    "# consecutive values in `days`\n",
    "dt = smax / 999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can obtain the probability density function of the exponential distribution with SciPy. \n",
    "#The parameter is the scale, the inverse of the estimated rate.\n",
    "dist_exp = st.expon.pdf(days, scale=1./ rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 30\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "ax.hist(survival, nbins)\n",
    "ax.plot(days, dist_exp * len(survival) * smax / nbins,\n",
    "        '-r', lw=3)\n",
    "ax.set_xlabel(\"Survival time (days)\")\n",
    "ax.set_ylabel(\"Number of patients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The fit is far from perfect\n",
    "- We were able to find an analytical formula for the maximum likelihood estimate here. In more complex situations, that is not always possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding numerical maximum likelihood using SciPy\n",
    "dist = st.expon\n",
    "args = dist.fit(survival)\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can use these parameters to perform a Kolmogorov-Smirnov test, which assesses the goodness of fit of the distribution with respect to the data. \n",
    "- The test is based on a distance between the empirical distribution function of the data and the cumulative distribution function (CDF) of the reference distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.kstest(survival, dist.cdf, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, the p-value is very low: the **null hypothesis** (stating that the observed data stems from an exponential distribution with a maximum likelihood rate parameter) can be rejected with high confidence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Birnbaum-Sanders Distribution\n",
    "- Typically used to model failure times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = st.fatiguelife\n",
    "args = dist.fit(survival)\n",
    "st.kstest(survival, dist.cdf, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This time, the p-value is about 0.073, so that we would not reject the null hypothesis with a five percent confidence level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_fl = dist.pdf(days, *args)\n",
    "nbins = 30\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "ax.hist(survival, nbins)\n",
    "ax.plot(days, dist_exp * len(survival) * smax / nbins,\n",
    "        '-r', lw=3, label='exp')\n",
    "ax.plot(days, dist_fl * len(survival) * smax / nbins,\n",
    "        '--g', lw=3, label='BS')\n",
    "ax.set_xlabel(\"Survival time (days)\")\n",
    "ax.set_ylabel(\"Number of patients\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math Stuff\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathcal{L}(\\lambda, \\{s_i\\}) &= P(\\{s_i\\} \\mid \\lambda) &\\\\\n",
    "&= \\prod_{i=1}^n P(s_i \\mid \\lambda) & \\textrm{(by independence of the $s_i$)}\\\\\n",
    "&= \\prod_{i=1}^n \\lambda \\exp(-\\lambda s_i) &\\\\\n",
    "&= \\lambda^n \\exp\\left(-\\lambda \\sum_{i=1}^n s_i\\right) &\\\\\n",
    "&= \\lambda^n \\exp\\left(-\\lambda n \\overline s\\right) &\n",
    "\\end{align*}$$\n",
    "\n",
    "### Find Max Using Derivative\n",
    "\n",
    "$$\\frac{d\\mathcal{L}(\\lambda, \\{s_i\\})}{d\\lambda} = \\lambda^{n-1} \\exp\\left(-\\lambda n \\overline s \\right) \\left( n - n \\lambda \\overline s \\right)$$\n",
    "\n",
    "$$\\lambda = \\frac {1}{\\overline s}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
