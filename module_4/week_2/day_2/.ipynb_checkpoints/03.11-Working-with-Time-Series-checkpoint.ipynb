{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pandas was developed in the context of financial modeling\n",
    "- Contains a fairly extensive set of tools for working with dates, times, and time-indexed data.\n",
    "\n",
    "- **Time stamps** reference particular moments in time (e.g., July 4th, 2015 at 7:00am).\n",
    "- **Time intervals** and *periods* reference a length of time between a particular beginning and end point; for example, the year 2015. Periods usually reference a special case of time intervals in which each interval is of uniform length and does not overlap (e.g., 24 hour-long periods comprising days).\n",
    "- **Time deltas** or *durations* reference an exact length of time (e.g., a duration of 22.56 seconds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates and Times in Python\n",
    "\n",
    "Python has a number of available representations of dates, times, deltas, and timespans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native Python dates and times: ``datetime`` and ``dateutil``\n",
    "\n",
    "- Python's basic objects for working with dates and times reside in the built-in ``datetime`` module.\n",
    "\n",
    "- Along with the third-party ``dateutil`` module, you can use it to quickly perform a host of useful functionalities on dates and times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "datetime(year=2015, month=7, day=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, using the ``dateutil`` module, you can parse dates from a variety of string formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "date = parser.parse(\"4th, july 2015\")\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a ``datetime`` object, you can do things like printing the day of the month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date.strftime('%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [strftime section](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior) \n",
    "\n",
    "- [datetime documentation](https://docs.python.org/3/library/datetime.html).\n",
    "\n",
    "- [useful date utilities](http://labix.org/python-dateutil).\n",
    "\n",
    "\n",
    "The power of ``datetime`` and ``dateutil`` lie in their flexibility and easy syntax: you can use these objects and their built-in methods to easily perform nearly any operation you might be interested in.\n",
    "\n",
    "\n",
    "Where they break down is when you wish to work with large arrays of dates and times:\n",
    "just as lists of Python numerical variables are suboptimal compared to NumPy-style typed numerical arrays, lists of Python datetime objects are suboptimal compared to typed arrays of encoded dates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typed arrays of times: NumPy's ``datetime64``\n",
    "- The ``datetime64`` requires a very specific input format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "date = np.array('2015-07-04', dtype=np.datetime64)\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have this date formatted, however, we can quickly do vectorized operations on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date - np.arange(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NumPy will infer the desired unit from the input; for example, here is a day-based datetime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.datetime64('2015-07-04')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is a minute-based datetime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.datetime64('2015-07-04 12:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the time zone is automatically set to the local time on the computer executing the code.\n",
    "You can force any desired fundamental time unit using one of many format codes; for example, here we'll force a nanosecond-based time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.datetime64('2015-07-04 12:59:59.50', 'ns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table, drawn from the [NumPy datetime64 documentation](http://docs.scipy.org/doc/numpy/reference/arrays.datetime.html), lists the available format codes along with the relative and absolute timespans that they can encode:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Code    | Meaning     | Time span (relative) | Time span (absolute)   |\n",
    "|--------|-------------|----------------------|------------------------|\n",
    "| ``Y``  | Year\t       | ± 9.2e18 years       | [9.2e18 BC, 9.2e18 AD] |\n",
    "| ``M``  | Month       | ± 7.6e17 years       | [7.6e17 BC, 7.6e17 AD] |\n",
    "| ``W``  | Week\t       | ± 1.7e17 years       | [1.7e17 BC, 1.7e17 AD] |\n",
    "| ``D``  | Day         | ± 2.5e16 years       | [2.5e16 BC, 2.5e16 AD] |\n",
    "| ``h``  | Hour        | ± 1.0e15 years       | [1.0e15 BC, 1.0e15 AD] |\n",
    "| ``m``  | Minute      | ± 1.7e13 years       | [1.7e13 BC, 1.7e13 AD] |\n",
    "| ``s``  | Second      | ± 2.9e12 years       | [ 2.9e9 BC, 2.9e9 AD]  |\n",
    "| ``ms`` | Millisecond | ± 2.9e9 years        | [ 2.9e6 BC, 2.9e6 AD]  |\n",
    "| ``us`` | Microsecond | ± 2.9e6 years        | [290301 BC, 294241 AD] |\n",
    "| ``ns`` | Nanosecond  | ± 292 years          | [ 1678 AD, 2262 AD]    |\n",
    "| ``ps`` | Picosecond  | ± 106 days           | [ 1969 AD, 1970 AD]    |\n",
    "| ``fs`` | Femtosecond | ± 2.6 hours          | [ 1969 AD, 1970 AD]    |\n",
    "| ``as`` | Attosecond  | ± 9.2 seconds        | [ 1969 AD, 1970 AD]    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the types of data we see in the real world, a useful default is ``datetime64[ns]``, as it can encode a useful range of modern dates with a suitably fine precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates and times in pandas: best of both worlds\n",
    "\n",
    "- Pandas builds upon all the tools just discussed to provide a ``Timestamp`` object, which combines the ease-of-use of ``datetime`` and ``dateutil`` with the efficient storage and vectorized interface of ``numpy.datetime64``.\n",
    "\n",
    "\n",
    "- From a group of these ``Timestamp`` objects, Pandas can construct a ``DatetimeIndex`` that can be used to index data in a ``Series`` or ``DataFrame``; we'll see many examples of this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "date = pd.to_datetime(\"4th of July, 2015\")\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date.strftime('%A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can do NumPy-style vectorized operations directly on this same object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date + pd.to_timedelta(np.arange(12), 'D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will take a closer look at manipulating time series data with the tools provided by Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Time Series: Indexing by Time\n",
    "\n",
    "- **Where the Pandas time series tools really become useful is when you begin to *index data by timestamps**.\n",
    "- For example, we can construct a ``Series`` object that has time indexed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.DatetimeIndex(['2014-07-04', '2014-08-04',\n",
    "                          '2015-07-04', '2015-08-04',\n",
    "                          '2016-07-04', '2016-08-04'])\n",
    "data = pd.Series([0, 1, 2, 3,4,5], index=index)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this data in a ``Series``, we can make use of any of the ``Series`` indexing patterns we discussed in previous sections, passing values that can be coerced into dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['2014-07-04':'2015-07-04']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are additional special date-only indexing operations, such as passing a year to obtain a slice of all data from that year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['2015']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, we will see additional examples of the convenience of dates-as-indices.\n",
    "But first, a closer look at the available time series data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Time Series Data Structures\n",
    "\n",
    "This section will introduce the fundamental Pandas data structures for working with time series data:\n",
    "\n",
    "- For **time stamps**, Pandas provides the ``Timestamp`` type. As mentioned before, it is essentially a replacement for Python's native ``datetime``, but is based on the more efficient ``numpy.datetime64`` data type. The associated Index structure is ``DatetimeIndex``.\n",
    "- For **time Periods**, Pandas provides the ``Period`` type. This encodes a fixed-frequency interval based on ``numpy.datetime64``. The associated index structure is ``PeriodIndex``.\n",
    "- For **time deltas** or **durations**, Pandas provides the ``Timedelta`` type. ``Timedelta`` is a more efficient replacement for Python's native ``datetime.timedelta`` type, and is based on ``numpy.timedelta64``. The associated index structure is ``TimedeltaIndex``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The most fundamental of these date/time objects are the ``Timestamp`` and ``DatetimeIndex`` objects.\n",
    "- Passing a single date to ``pd.to_datetime()`` yields a ``Timestamp``; \n",
    "- Passing a series of dates by default yields a ``DatetimeIndex``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.to_datetime([datetime(2015, 7, 3), '4th of July, 2015',\n",
    "                       '2015-Jul-6', '07-07-2015', '20150708'])\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any ``DatetimeIndex`` can be converted to a ``PeriodIndex`` with the ``to_period()`` function with the addition of a frequency code; here we'll use ``'M'`` to indicate monthly frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates.to_period('m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ``TimedeltaIndex`` is created, for example, when a date is subtracted from another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates - dates[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular sequences: ``pd.date_range()``\n",
    "\n",
    "To make the creation of regular date sequences more convenient, Pandas offers a few functions for this purpose: \n",
    "- ``pd.date_range()`` for timestamps \n",
    "- ``pd.period_range()`` for periods\n",
    "- ``pd.timedelta_range()`` for time deltas.\n",
    "- ``pd.date_range()`` accepts a start date, an end date, and an optional frequency code to create a regular sequence of dates.\n",
    "**By default, the frequency is one day**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range('2015-07-03', '2015-07-10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the date range can be specified not with a start and endpoint, but with a startpoint and a number of periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range('2015-07-03', periods=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spacing can be modified by altering the ``freq`` argument, which defaults to ``D``.\n",
    "For example, here we will construct a range of hourly timestamps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range('2015-07-03', periods=8, freq='H')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To **create** regular sequences of ``Period`` or ``Timedelta`` values, the very similar ``pd.period_range()`` and ``pd.timedelta_range()`` functions are useful.\n",
    "Here are some monthly periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.period_range('2015-07', periods=8, freq='M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a sequence of durations increasing by an hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.timedelta_range(0, periods=10, freq='H')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequencies and Offsets\n",
    "\n",
    "Fundamental to these Pandas time series tools is the concept of a frequency or date offset.\n",
    "Just as we saw the ``D`` (day) and ``H`` (hour) codes above, we can use such codes to specify any desired frequency spacing.\n",
    "The following table summarizes the main codes available:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Code   | Description         | Code   | Description          |\n",
    "|--------|---------------------|--------|----------------------|\n",
    "| ``D``  | Calendar day        | ``B``  | Business day         |\n",
    "| ``W``  | Weekly              |        |                      |\n",
    "| ``M``  | Month end           | ``BM`` | Business month end   |\n",
    "| ``Q``  | Quarter end         | ``BQ`` | Business quarter end |\n",
    "| ``A``  | Year end            | ``BA`` | Business year end    |\n",
    "| ``H``  | Hours               | ``BH`` | Business hours       |\n",
    "| ``T``  | Minutes             |        |                      |\n",
    "| ``S``  | Seconds             |        |                      |\n",
    "| ``L``  | Milliseonds         |        |                      |\n",
    "| ``U``  | Microseconds        |        |                      |\n",
    "| ``N``  | nanoseconds         |        |                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The monthly, quarterly, and annual frequencies are all marked at the end of the specified period.\n",
    "By adding an ``S`` suffix to any of these, they instead will be marked at the beginning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Code    | Description            || Code    | Description            |\n",
    "|---------|------------------------||---------|------------------------|\n",
    "| ``MS``  | Month start            ||``BMS``  | Business month start   |\n",
    "| ``QS``  | Quarter start          ||``BQS``  | Business quarter start |\n",
    "| ``AS``  | Year start             ||``BAS``  | Business year start    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you can change the month used to mark any quarterly or annual code by adding a three-letter month code as a suffix:\n",
    "\n",
    "- ``Q-JAN``, ``BQ-FEB``, ``QS-MAR``, ``BQS-APR``, etc.\n",
    "- ``A-JAN``, ``BA-FEB``, ``AS-MAR``, ``BAS-APR``, etc.\n",
    "\n",
    "In the same way, the split-point of the weekly frequency can be modified by adding a three-letter weekday code:\n",
    "\n",
    "- ``W-SUN``, ``W-MON``, ``W-TUE``, ``W-WED``, etc.\n",
    "\n",
    "**On top of this, codes can be combined with numbers to specify other frequencies.\n",
    "For example, for a frequency of 2 hours 30 minutes, we can combine the hour (``H``) and minute (``T``) codes as follows:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.timedelta_range(0, periods=9, freq=\"2H30T\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these short codes refer to specific instances of Pandas time series offsets, which can be found in the ``pd.tseries.offsets`` module.\n",
    "- For example, we can create a business day offset directly as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.offsets import BDay\n",
    "pd.date_range('2015-07-01', periods=5, freq=BDay())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more discussion of the use of frequencies and offsets, see the [\"DateOffset\" section](http://pandas.pydata.org/pandas-docs/stable/timeseries.html#dateoffset-objects) of the Pandas documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling, Shifting, and Windowing\n",
    "\n",
    "The ability to use dates and times as indices to intuitively organize and **access data** is an important piece of the Pandas time series tools.\n",
    "\n",
    "For example, the accompanying ``pandas-datareader`` package (installable via ``conda install pandas-datareader``), knows how to import financial data from a number of available sources, including Yahoo finance, Google Finance, and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas_datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data\n",
    "\n",
    "goog = data.DataReader('GOOG',start='2004', end='2019', data_source='yahoo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we'll use just the closing price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goog = goog['Close']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize this using the ``plot()`` method, after the normal Matplotlib setup boilerplate (see [Chapter 4](04.00-Introduction-To-Matplotlib.ipynb)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel('Price')\n",
    "goog.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling and converting frequencies\n",
    "\n",
    "- Resampling involves changing the frequency of your time series observations.\n",
    "\n",
    "**Two types of resampling are:**\n",
    "\n",
    "- **Upsampling**: Where you increase the frequency of the samples, such as from minutes to seconds.\n",
    "\n",
    "\n",
    "- **Downsampling**: Where you decrease the frequency of the samples, such as from days to months.\n",
    "\n",
    "\n",
    "\n",
    "- This can be done using the ``resample()`` method, or the much simpler ``asfreq()`` method.\n",
    "- The primary difference between the two is that ``resample()`` is fundamentally a *data aggregation*\n",
    "\n",
    "- while ``asfreq()`` is fundamentally a *data selection*.\n",
    "\n",
    "Taking a look at the Google closing price, let's compare what the two return when we down-sample the data.\n",
    "Here we will resample the data at the end of business year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goog.plot(alpha=0.5, style='-')\n",
    "goog.resample('BA').mean().plot(style=':')\n",
    "goog.asfreq('BA').plot(style='--');\n",
    "plt.legend(['input', 'resample', 'asfreq'],\n",
    "           loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the difference: \n",
    "- At each point, ``resample`` reports the *average of the previous year*\n",
    "\n",
    "- While ``asfreq`` reports the *value at the end of the year*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For up-sampling, ``resample()`` and ``asfreq()`` are largely equivalent, though resample has many more options available.\n",
    "\n",
    "\n",
    "- ``asfreq()`` accepts a ``method`` argument to specify how values are imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the business day data at a daily frequency\n",
    "fig, ax = plt.subplots(2, sharex=True)\n",
    "data = goog.iloc[:10]\n",
    "\n",
    "data.asfreq('D').plot(ax=ax[0], marker='o')\n",
    "\n",
    "data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')\n",
    "data.asfreq('D', method='ffill').plot(ax=ax[1], style='--o')\n",
    "ax[1].legend([\"back-fill\", \"forward-fill\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top panel is the default: non-business days are left as NA values and do not appear on the plot.\n",
    "The bottom panel shows the differences between two strategies for filling the gaps: forward-filling and backward-filling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-shifts\n",
    "\n",
    "- Another common time series-specific operation is shifting of data in time.\n",
    "Pandas has two closely related methods for computing this: ``shift()`` and ``tshift()``\n",
    "- ``shift()`` *shifts the data*, while \n",
    "- ``tshift()`` *shifts the index*.\n",
    "In both cases, the shift is specified in multiples of the frequency.\n",
    "\n",
    "Here we will both ``shift()`` and ``tshift()`` by 900 days; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, sharey=True)\n",
    "\n",
    "# apply a frequency to the data\n",
    "goog = goog.asfreq('D', method='pad')\n",
    "\n",
    "goog.plot(ax=ax[0])\n",
    "goog.shift(900).plot(ax=ax[1])\n",
    "goog.tshift(900).plot(ax=ax[2])\n",
    "\n",
    "# legends and annotations\n",
    "local_max = pd.to_datetime('2007-11-05')\n",
    "offset = pd.Timedelta(900, 'D')\n",
    "\n",
    "ax[0].legend(['input'], loc=2)\n",
    "ax[0].get_xticklabels()[2].set(weight='heavy', color='red')\n",
    "ax[0].axvline(local_max, alpha=0.3, color='red')\n",
    "\n",
    "ax[1].legend(['shift(900)'], loc=2)\n",
    "ax[1].get_xticklabels()[2].set(weight='heavy', color='red')\n",
    "ax[1].axvline(local_max + offset, alpha=0.3, color='red')\n",
    "\n",
    "ax[2].legend(['tshift(900)'], loc=2)\n",
    "ax[2].get_xticklabels()[1].set(weight='heavy', color='red')\n",
    "ax[2].axvline(local_max + offset, alpha=0.3, color='red');\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ``shift(900)`` shifts the *data* by 900 days, pushing some of it off the end of the graph (and leaving NA values at the other end),\n",
    "- ``tshift(900)`` shifts the *index values* by 900 days.\n",
    "\n",
    "A common context for this type of shift is in computing differences over time. For example, we use shifted values to compute the one-year return on investment for Google stock over the course of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI = 100 * (goog.tshift(-365) / goog - 1)\n",
    "ROI.plot()\n",
    "plt.ylabel('% Return on Investment');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helps us to see the overall trend in Google stock: \n",
    "- The most profitable times to invest in Google have been shortly after its IPO\n",
    "- The middle of the 2009 recession."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling windows\n",
    "\n",
    "- Rolling statistics are a third type of time series-specific operation implemented by Pandas.\n",
    "- These can be accomplished via the ``rolling()`` attribute of ``Series`` and ``DataFrame`` objects, which returns a view similar to what we saw with the ``groupby`` operation\n",
    "- The rolling view makes available a number of aggregation operations by default.\n",
    "\n",
    "For example, here is the one-year centered rolling mean and standard deviation of the Google stock prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling = goog.rolling(365, center=True)\n",
    "\n",
    "data = pd.DataFrame({'input': goog,\n",
    "                     'one-year rolling_mean': rolling.mean(),\n",
    "                     'one-year rolling_std': rolling.std()})\n",
    "ax = data.plot(style=['-', '--', ':'])\n",
    "ax.lines[0].set_alpha(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with group-by operations, the ``aggregate()`` and ``apply()`` methods can be used for custom rolling computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to Learn More\n",
    "\n",
    "This section has provided only a brief summary of some of the most essential features of time series tools provided by Pandas; for a more complete discussion, you can refer to the [\"Time Series/Date\" section](http://pandas.pydata.org/pandas-docs/stable/timeseries.html) of the Pandas online documentation.\n",
    "\n",
    "Another excellent resource is the textbook [Python for Data Analysis](http://shop.oreilly.com/product/0636920023784.do) by Wes McKinney (OReilly, 2012).\n",
    "Although it is now a few years old, it is an invaluable resource on the use of Pandas.\n",
    "In particular, this book emphasizes time series tools in the context of business and finance, and focuses much more on particular details of business calendars, time zones, and related topics.\n",
    "\n",
    "As always, you can also use the IPython help functionality to explore and try further options available to the functions and methods discussed here. I find this often is the best way to learn a new Python tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Visualizing Seattle Bicycle Counts\n",
    "\n",
    "As a more involved example of working with some time series data, let's take a look at bicycle counts on Seattle's [Fremont Bridge](http://www.openstreetmap.org/#map=17/47.64813/-122.34965).\n",
    "This data comes from an automated bicycle counter, installed in late 2012, which has inductive sensors on the east and west sidewalks of the bridge.\n",
    "The hourly bicycle counts can be downloaded from http://data.seattle.gov/; here is the [direct link to the dataset](https://data.seattle.gov/Transportation/Fremont-Bridge-Hourly-Bicycle-Counts-by-Month-Octo/65db-xm6k).\n",
    "\n",
    "As of summer 2016, the CSV can be downloaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o FremontBridge.csv https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this dataset is downloaded, we can use Pandas to read the CSV output into a ``DataFrame``.\n",
    "We will specify that we want the Date as an index, and we want these dates to be automatically parsed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('FremontBridge.csv', index_col='Date', parse_dates=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we'll further process this dataset by shortening the column names and adding a \"Total\" column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['West', 'East']\n",
    "data['Total'] = data.eval('West + East')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the summary statistics for this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data\n",
    "\n",
    "We can gain some insight into the dataset by visualizing it.\n",
    "Let's start by plotting the raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn; seaborn.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot()\n",
    "plt.ylabel('Hourly Bicycle Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ~25,000 hourly samples are far too dense for us to make much sense of.\n",
    "We can gain more insight by resampling the data to a coarser grid.\n",
    "Let's resample by week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly = data.resample('W').sum()\n",
    "weekly.plot(style=[':', '--', '-'])\n",
    "plt.ylabel('Weekly bicycle count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us some interesting seasonal trends: as you might expect, people bicycle more in the summer than in the winter, and even within a particular season the bicycle use varies from week to week (likely dependent on weather; see [In Depth: Linear Regression](05.06-Linear-Regression.ipynb) where we explore this further).\n",
    "\n",
    "Another way that comes in handy for aggregating the data is to use a rolling mean, utilizing the ``pd.rolling_mean()`` function.\n",
    "Here we'll do a 30 day rolling mean of our data, making sure to center the window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = data.resample('D').sum()\n",
    "daily.rolling(30, center=True).sum().plot(style=[':', '--', '-'])\n",
    "plt.ylabel('mean hourly count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The jaggedness of the result is due to the hard cutoff of the window.\n",
    "We can get a smoother version of a rolling mean using a window function–for example, a Gaussian window.\n",
    "The following code specifies both the width of the window (we chose 50 days) and the width of the Gaussian within the window (we chose 10 days):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily.rolling(50, center=True,\n",
    "              win_type='gaussian').sum(std=10).plot(style=[':', '--', '-']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digging into the data\n",
    "\n",
    "While these smoothed data views are useful to get an idea of the general trend in the data, they hide much of the interesting structure.\n",
    "For example, we might want to look at the average traffic as a function of the time of day.\n",
    "We can do this using the GroupBy functionality discussed in [Aggregation and Grouping](03.08-Aggregation-and-Grouping.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_time = data.groupby(data.index.time).mean()\n",
    "hourly_ticks = 4 * 60 * 60 * np.arange(6)\n",
    "by_time.plot(xticks=hourly_ticks, style=[':', '--', '-']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hourly traffic is a strongly bimodal distribution, with peaks around 8:00 in the morning and 5:00 in the evening.\n",
    "This is likely evidence of a strong component of commuter traffic crossing the bridge.\n",
    "This is further evidenced by the differences between the western sidewalk (generally used going toward downtown Seattle), which peaks more strongly in the morning, and the eastern sidewalk (generally used going away from downtown Seattle), which peaks more strongly in the evening.\n",
    "\n",
    "We also might be curious about how things change based on the day of the week. Again, we can do this with a simple groupby:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_weekday = data.groupby(data.index.dayofweek).mean()\n",
    "by_weekday.index = ['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun']\n",
    "by_weekday.plot(style=[':', '--', '-']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows a strong distinction between weekday and weekend totals, with around twice as many average riders crossing the bridge on Monday through Friday than on Saturday and Sunday.\n",
    "\n",
    "With this in mind, let's do a compound GroupBy and look at the hourly trend on weekdays versus weekends.\n",
    "We'll start by grouping by both a flag marking the weekend, and the time of day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekend = np.where(data.index.weekday < 5, 'Weekday', 'Weekend')\n",
    "by_time = data.groupby([weekend, data.index.time]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use some of the Matplotlib tools described in [Multiple Subplots](04.08-Multiple-Subplots.ipynb) to plot two panels side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "by_time.ix['Weekday'].plot(ax=ax[0], title='Weekdays',\n",
    "                           xticks=hourly_ticks, style=[':', '--', '-'])\n",
    "by_time.ix['Weekend'].plot(ax=ax[1], title='Weekends',\n",
    "                           xticks=hourly_ticks, style=[':', '--', '-']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is very interesting: we see a bimodal commute pattern during the work week, and a unimodal recreational pattern during the weekends.\n",
    "It would be interesting to dig through this data in more detail, and examine the effect of weather, temperature, time of year, and other factors on people's commuting patterns; for further discussion, see my blog post [\"Is Seattle Really Seeing an Uptick In Cycling?\"](https://jakevdp.github.io/blog/2014/06/10/is-seattle-really-seeing-an-uptick-in-cycling/), which uses a subset of this data.\n",
    "We will also revisit this dataset in the context of modeling in [In Depth: Linear Regression](05.06-Linear-Regression.ipynb)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
